# java config property file
# config the project
maxFetchingThread=50
maxParseThread=50

pagesToFetch=1200000
#pagesToFetch=2000

# database config
dbHost=127.0.0.1
dbPort=13306
dbUsername=webCrawler
dbPassword=webCrawler
dbName=webCrawler

# trimTags=css-script
isTrimTags=true


# auto change ip address
isAutoChangeIp=true

# proxy config
useProxy=true
proxyHost=10.4.20.2
proxyPort=3128
proxyUsername=""
proxyPassword=""


# log config
useFileLog=true
# logFile=log/crawler.log

# save status
isSaveStatus=true
# statusFolder=status
isResume=false

# save urls resume to one file, for now false is not supported
isResumeFromOneFile=true

# url regular expression
urlPattern=.*

# these file type will be ignored, use lowerCase
excludeType=jpg,jpeg,pdf,apk,zip,rar,7z,tar,gz,2z,gif,ttf,swf,doc